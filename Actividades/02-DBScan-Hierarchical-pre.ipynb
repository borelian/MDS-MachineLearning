{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Actividad DBSCAN y hierarchical clustering (análisis)\n",
    "\n",
    "En esta actividad se debe aplicar dbscan y hierarchical clustering para unos datos de videojuegos e interpretar sus resultados. Como ejemplo, se presenta los resultados para el modelo de k-means."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Instalando las librerias requeridas\n",
    "#import sys\n",
    "#!{sys.executable} -m pip install numpy, pandas, plotnine, sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from plotnine import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Los datos corresponden a distintas ventas de videojuegos en el mundo, así como las evaluaciones de la crítica y usuarios y otros features. Para esta actividad, nos limitaremos a usar los datos númericos, que corresponden a las ventas y las críticas.\n",
    "\n",
    "También, escalaremos los datos entre 0 y 1, usando la función `MinMaxScaler` de sklearn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cargando set de datos y haciendo limpieza\n",
    "origData=pd.read_csv(\"video_games_sales.csv\")\n",
    "origData=origData.dropna() #Eliminando datos con valores NA \n",
    "\n",
    "#eliminando las variables discretas y transformando el resto a númerico\n",
    "origDataNumeric=origData.iloc[:,[5,6,7,8,10,12]].copy()\n",
    "\n",
    "#Estandarizando las variables entre 0 y 1\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "scaler = MinMaxScaler()\n",
    "newData = pd.DataFrame(scaler.fit_transform(origDataNumeric), index = origDataNumeric.index, columns=origDataNumeric.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "newData"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apliquemos K-Means para ver el resultado que nos entrega."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Aplicando K-means\n",
    "from sklearn.cluster import KMeans\n",
    "sse = []\n",
    "maxK=20\n",
    "for k in range(1, maxK):\n",
    "    kmeans = KMeans(n_clusters=k)\n",
    "    kmeans.fit(newData)\n",
    "    sse.append(kmeans.inertia_)\n",
    "pd.DataFrame(sse, index=np.arange(1,maxK)).plot()\n",
    "# tempDF = pd.DataFrame(range(1, maxK),columns=[\"numK\"])\n",
    "# tempDF[\"sse\"]=sse\n",
    "# (ggplot(tempDF)+aes(x=\"numK\",y=\"sse\")+theme_bw()+geom_line()+labs(x=\"Número de clusters\",y=\"WCD\")\n",
    "#  +scale_x_continuous(breaks=range(1,maxK)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Seleccionando entre 5 y 10 clusters\n",
    "numK=5\n",
    "finalModel=KMeans(n_clusters=numK)\n",
    "finalModel=finalModel.fit(newData)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Veamos las estadísticas de cada cluster (media y varianza)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "newData.groupby(finalModel.labels_).agg(['mean','std'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finalmente, intentemos visualizar los clusters usando PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Visualizando los clusters con PCA\n",
    "from sklearn.decomposition import PCA\n",
    "pca = PCA(n_components=2)\n",
    "pca = pca.fit(newData)\n",
    "tempData = pca.transform(newData)\n",
    "tempData = pd.DataFrame(tempData,columns=[\"PC1\",\"PC2\"])\n",
    "tempData[\"labels\"]=finalModel.labels_\n",
    "ggplot(tempData)+aes(x=\"PC1\",y=\"PC2\",color=\"factor(labels)\")+geom_point(alpha=0.6)+theme_bw()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datapc = pd.DataFrame(pca.components_.transpose(),columns=[\"PC1\",\"PC2\"])\n",
    "datapc['varNames']=newData.columns\n",
    "ggplot(datapc, aes(x='PC1',y='PC2')) + geom_segment(xend=0, yend=0, arrow=arrow(ends='first'), color='blue')\\\n",
    "+ geom_text(aes(label='varNames'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Paso 1: Interpretación K-Means\n",
    "## Describa las características importantes de cada cluster.\n",
    "Apoyese en las estadisticas de cada cluster.  Observe que las PCA están basadas principalmente en los scores de usuarios y de la crítica."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Paso 2 :  DBScan\n",
    "## Aplique DBSCAN, encuentre el mejor eps para minPts=7 y explique los clusters encontrados\n",
    "\n",
    "Primero, determinemos el valor de `eps` a utilizar en DBSCAN usando la función de `NearestNeighbors`. Usaremos 7 vecinos para esto."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import NearestNeighbors\n",
    "numNeighboors=7\n",
    "neighbors = NearestNeighbors(n_neighbors=numNeighboors) #Creando el modelo\n",
    "\n",
    "# Entrene el modelo con los datos (.fit())\n",
    "\n",
    "\n",
    "#Use la función kneighbors de neighbors_fit con newData para que retorne las distancis e indices\n",
    "\n",
    "# OJO: recuerde que .kneighbors() retorna todas las distancias,\n",
    "# para recuperar las que nos interesan, usar distances[:, numNeighboors-1]\n",
    "\n",
    "#Ordene las distancias obtenidas usando la función sort del vector\n",
    "\n",
    "# Grafique las distancias.\n",
    "# Ejemplo, si sus distancias están en un arreglo llamada `distances`, puede usar\n",
    "#   pd.DataFrame(distances, index=np.arange(1,len(distances)+1)).plot()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Paso 3: Ejecute DBScan con el valor de eps seleccionado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creando un objeto de DBSCAN con las condiciones iniciales\n",
    "\n",
    "from sklearn.cluster import DBSCAN\n",
    "finalModel = DBSCAN(eps= ... , min_samples=7)\n",
    "finalModel.fit(....)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Revisemos el número de clusters obtenidos, así como el numero de datos en cada cluster\n",
    "np.unique(finalModel.labels_, return_counts=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Paso 4: Interpretación de DBSCAN\n",
    "### Interprete los clusters. Describa los clusters obtenidos. ¿Difieren de K-Medias?\n",
    "\n",
    "Para esto, copie los analisis anteriores de medias por cluster, así como su visualización en las PCA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# (Recordatorio de comandos)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DBSCAN\n",
    "DBSCAN(eps=0.5,min_samples=5, metric='euclidean')<br><br>\n",
    "Parámetros\n",
    "* eps: radio de la esfera n-dimensional para la cual se buscan los minPoints.\n",
    "* minPts: número mínimo de puntos dentro de la región definida por eps para considerar a un punto como crítico (el punto a analizar también se cuenta).\n",
    "* metric: métrica para el cálculo de la distancia<br> \n",
    "scikit-learn => ‘cityblock’, ‘cosine’, ‘euclidean’,‘manhattan’ <br>\n",
    "scipy.spatial.distance => ‘chebyshev’, ‘correlation’, ‘hamming’, ‘jaccard’, ‘mahalanobis’, ‘minkowski’, ‘seuclidean’,’sqeuclidean’.<br><br>\n",
    "\n",
    "Atributos\n",
    "* core_sample_indices_: indices de puntos core.\n",
    "* components_: Copia de los puntos core.\n",
    "* labels_: etiquetas de los puntos (-1 implica outlier).<br><br>\n",
    "\n",
    "Funciones:\n",
    "* fit(X): entrena el modelo con los parametros asignados.\n",
    "* fit_predict(X): entrena y devuelve los clusters encontrados con los parametros asignados."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# clase NearestNeighbors\n",
    "NearestNeighbors(n_neighbors=5, metric='minkowski', p=2)<br>\n",
    "Parámetros\n",
    "* n_neighborsint: Números de vecinos a estimar.\n",
    "* metric: métrica para el cálculo de la distancia.\n",
    "* p: parámetro de la distancia de minkowski (2 es euclideana). <br><br>\n",
    "\n",
    "Atributos\n",
    "* effective_metric_: métrica usada para el cálculo de la distancia.\n",
    "* effective_metric_params_: Parámetros de la métrica usada para el cálculo de la distancia.\n",
    "* n_samples_fit_: número de puntos del dataset.<br><br>\n",
    "\n",
    "Funciones:\n",
    "* fit(X): Encontrar los vecinos más cercanos de los datos.\n",
    "* kneighbors(X, n_neighbors): Encontrar los K vecinos más cercano de un punto, retorna tanto los índices como la distancia.\n",
    "* radius_neighbors(X, radius): Encontrar los vecinos de uno o más puntos que se encuentran dentro de un radio determinado."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Paso 5: AgglomerativeClustering\n",
    "\n",
    "Repitamos ahora esto usando cluster jerárquicos.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dibuje el dendograma de los datos con la función `linkage`.\n",
    "Pruebe con distintos métodos de calcular la distancia entre cluster\n",
    "\n",
    "method: ‘ward’, ‘complete’, ‘average’, ‘single’`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.cluster.hierarchy as shc\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "#Entrenando el modelo\n",
    "modelo = shc.linkage(newData, method= ...)\n",
    "plt.figure(figsize=(10, 7)) #Seteando el tamaño de la figura\n",
    "\n",
    "### Hint: cuando graficamos el dendrograma use truncate_mode=\"lastp\" para evitar problemas de memoria.\n",
    "\n",
    "objeto = shc.dendrogram(modelo, truncate_mode=\"lastp\", p=100) #Generando el dendrograma"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Decida un límite en la distancia para cortar el dendograma y generar los clusters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clusters=shc.fcluster(modelo,t=...,criterion=\"distance\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.unique(clusters, return_counts=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Paso 6: Interpretación de AglomerativeClustering\n",
    "### Interprete los clusters. Describa los clusters obtenidos. ¿Difieren de K-Medias? ¿de DBSCAN? ¿Cuál parece mas adecuado?\n",
    "\n",
    "Para esto, copie los analisis anteriores de medias por cluster, así como su visualización en las PCA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AgglomerativeClustering\n",
    "\n",
    "sklearn tiene varias deficiencias para este modelo. Siendo las más importantes, el definir un número de cluster y no generar un dendrograma en forma sencilla.<br>\n",
    "\n",
    "Utilizaremos las funciones linkage, dendrogram y dendrogram de la biblioteca scipy.cluster.hierarchy.<br>\n",
    "\n",
    "La función linkage aprende el modelo definido y retorna un dendrograma<br>\n",
    "linkage(y, method='single')<br>\n",
    "Parámetros:\n",
    "* y: son los datos del modelo\n",
    "* method: ‘ward’, ‘complete’, ‘average’, ‘single’.<br><br>\n",
    "\n",
    "Para graficar basta con llamar a la función dendrogram con el modelo aprendido<br>dendrogram(modeloAprendido,p=30,truncate_mode=None)<br>\n",
    "donde truncate_mode puede ser None (muestra todo el dendrograma) o 'lastp' (corta el dendrograma después de p ramas).\n",
    "\n",
    "Para extraer los clusters se utiliza la función fcluster\n",
    "fcluster(Z, t, criterion)<br>\n",
    "Parámetros:\n",
    "* Z: el modelo generardo por la función linkage\n",
    "* t: el valor donde se quiere cortar el dendrograma\n",
    "* criterion=“distance”. existen otros criterios de corte, pero distance es el que nos permite generar los clusters, basados en la altura que definimos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python3.9",
   "language": "python",
   "name": "python3.9"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
