{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Actividad árbol de decision \n",
    "En esta actividad, repetiremos el análisis para detección de cancer, usando esta vez Arboles de decisión"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from plotnine import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_breast_cancer\n",
    "loadData = load_breast_cancer(as_frame=True)\n",
    "data = loadData['data']\n",
    "data.columns = data.columns.str.replace(' ','_')\n",
    "target = loadData['target']\n",
    "target = 1-target # para que \"1\" sea \"Maligno\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Separamos los datos en un set de entrenamiento (`X_train` y `y_train`) para calibrar el modelo, y un set de test (`X_test`, `y_test`) para evaluarlo. (Ojo: no necesitamos normalizar los datos para este método)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(data, target, random_state=0)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parte 1:  Entrenar un modelo de Arbol de Decisión y evaluar su performance.\n",
    "\n",
    "Genere un modelo de Arbol de Decisión y calibrelo para los datos de entrenamiento.  Con esto haga una predicción para los datos de `X_test`, y guarde estos resultados en la variable `prediccion`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "AD = DecisionTreeClassifier()\n",
    "AD = AD.fit(<Datos entrenamiento>, <Target entrenamiento>)\n",
    "prediccion=NB.predict(<Datos test>)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evalue el performance de su modelo, para las distintas métricas. Para esto, recuerde comparar los resultados reales `y_test` con los resultados predecidos `prediccion`.\n",
    "\n",
    "Recuerde que puede usar distintas herramientas, por ejemplo:\n",
    "- Accuracy : `mt.accuracy_score(<reales>,<prediccion>)` \n",
    "- Precision: `mt.precision_score(<reales>,<prediccion>)` \n",
    "- Recall: `mt.recall_score(<reales>,<prediccion>)` \n",
    "- F1: `mt.f1_score(<reales>,<prediccion>)` \n",
    "- Reporte general: `mt.classification_report(<reales>,<prediccion>)`\n",
    "- Matriz confusión: `mt.confusion_matrix(<reales>,<prediccion>)`\n",
    "- Matriz confusión (plot): `mt.ConfusionMatrixDisplay(mt.confusion_matrix(<reales>,<prediccion>)).plot()`\n",
    "\n",
    "Recuerde F1= 2TP/(2TP+FN+FP), recall= TP/(TP+FP) y precision= TP/(TP+FN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics as mt\n",
    "# Escriba aquí sus evaluaciones\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parte 2 : Repetir la evaluación usando cross-validation\n",
    "Para major certeza de lo obtenido, haremos un `KFold` (sugerencia: usar 10-fold), y usaremos `cross_validate` para evaluar cada una de las métricas, reportando su media y desviación estandar.  Para esto usamos los datos completos (o sea, no `X_train`, ...) ya que `cross_validate` se encargará de separar el conjunto train y test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "from sklearn import metrics as mt\n",
    "from sklearn.model_selection import cross_validate\n",
    "\n",
    "\n",
    "#Creamos un objeto de la clase KFold \n",
    "kf = KFold(n_skf = KFold(n_splits= <NUMERO DE FOLDS ELEGIDO> )\n",
    "\n",
    "    \n",
    "#Cree un objeto de la clase DecisionTreeClassifier. No necesita hacer .fit, cross_validate se encarga de eso\n",
    "AD = DecisionTreeClassifier()\n",
    "\n",
    "\n",
    "#Hacemos cross_validate para las distintas métricas. \n",
    "for metrica in ['accuracy', 'recall', 'precision', 'f1']:\n",
    "    res = cross_validate(AD, <DATOS ORIGINALES>, <TARGET ORIGINAL>, scoring=metrica, cv=kf, return_train_score=True)    \n",
    "    print ('Score',metrica,'train:', res['train_score'].mean(),'+-',res['train_score'].std())\n",
    "    print ('Score',metrica,'test:', res['test_score'].mean(),'+-',res['test_score'].std())\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parte 3 : Calibrar el modelo para distintas tamaños de arbol usando cross-validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creando el objeto con sus características \n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn import metrics as mt\n",
    "\n",
    "\n",
    "# Defina la profundidad máxima a explorar\n",
    "maxH= <SU PROFUNDIDAD MÁXIMA>\n",
    "\n",
    "#Generando los k-fold\n",
    "kf = KFold(n_splits=10)\n",
    "\n",
    "# guardaremos los resultados acá\n",
    "results = pd.DataFrame(columns=['depth','Tipo', 'Metrica', 'ScoreMean', 'ScoreStd'])\n",
    "\n",
    "for k in range(1,maxH+1): #Ciclo for para buscar el mejor valor de max_depth\n",
    "    #Cree un objeto de la clase KNeighborsClassifier con n_neighbors=k\n",
    "    AD = DecisionTreeClassifier(max_depth=k)\n",
    "    #Hacemos un ciclo for para las distintas métricas\n",
    "    for metrica in ['accuracy', 'recall', 'precision', 'f1']:\n",
    "        res = cross_validate(AD, <DATOS ORIGINALES>, <TARGET ORIGINAL>, scoring=metrica, cv=kf, return_train_score=True)    \n",
    "        results.loc[len(results)] = [k,'train',metrica,res['train_score'].mean(),res['train_score'].std()]\n",
    "        results.loc[len(results)] = [k,'test',metrica,res['test_score'].mean(),res['test_score'].std()]\n",
    "\n",
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Si todo resultó bien, grafique los resultados con el siguiente código:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ggplot(results, aes(x='depth', color='Tipo'))\\\n",
    "+ geom_line(aes(y='ScoreMean', group='Tipo'))\\\n",
    "+ geom_errorbar(aes(ymin='ScoreMean-ScoreStd', ymax='ScoreMean+ScoreStd'))\\\n",
    "+ facet_wrap('Metrica')\\\n",
    "+ theme_bw()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Pregunta**: ¿Cuál es la mejor profundidad máxima a su parecer?  Escoga una como \"la mejor\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parte 4: Comparación con los métodos anteriores\n",
    "Comparemos el performance obtenido para las distintas métricas, con los modelo KNN y GaussianNB anteriormente ajustados.  Repita el comando, usando `DecisionTreeClassifier` esta vez para la profundidad escogida"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#Creando el objeto con sus características \n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn import metrics as mt\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "\n",
    "scaler = StandardScaler()\n",
    "dataNorm = scaler.fit_transform(data)\n",
    "\n",
    "results = pd.DataFrame(columns=['Metodo','Tipo', 'Metrica', 'ScoreMean', 'ScoreStd'])\n",
    "\n",
    "#Cree un objeto de la clase KNeighborsClassifier con n_neighbors=k\n",
    "KNNmodel = KNeighborsClassifier(n_neighbors=<SU NUMERO DE VECINOS PARA KNN>)\n",
    "for metrica in ['accuracy', 'recall', 'precision', 'f1']:\n",
    "    res = cross_validate(KNNmodel, dataNorm, target, cv=kf, return_train_score=True, scoring=metrica)    \n",
    "    results.loc[len(results)] = ['KNN','train',metrica,res['train_score'].mean(),res['train_score'].std()]\n",
    "    results.loc[len(results)] = ['KNN','test',metrica,res['test_score'].mean(),res['test_score'].std()]\n",
    "\n",
    "NB = GaussianNB()\n",
    "for metrica in ['accuracy', 'recall', 'precision', 'f1']:\n",
    "    res = cross_validate(NB, data,target, cv=kf, return_train_score=True, scoring=metrica)\n",
    "    results.loc[len(results)] = ['NB','train',metrica,res['train_score'].mean(),res['train_score'].std()]\n",
    "    results.loc[len(results)] = ['NB','test',metrica,res['test_score'].mean(),res['test_score'].std()]\n",
    "\n",
    "# Agregue la profundidad escogida\n",
    "AD = DecisionTreeClassifier(max_depth=<PROFUNDIDAD_ESCOGIDA>)\n",
    "for metrica in ['accuracy', 'recall', 'precision', 'f1']:\n",
    "    res = cross_validate(AD, data,target, cv=kf, return_train_score=True, scoring=metrica)\n",
    "    results.loc[len(results)] = ['AD','train',metrica,res['train_score'].mean(),res['train_score'].std()]\n",
    "    results.loc[len(results)] = ['AD','test',metrica,res['test_score'].mean(),res['test_score'].std()]\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(ggplot(results)+\n",
    " aes(x='Metodo',y=\"ScoreMean\",ymin=\"ScoreMean-ScoreStd\",ymax=\"ScoreMean+ScoreStd\",color='Tipo')+\n",
    " geom_point()+ geom_errorbar()+ \n",
    " labs(y=\"Score\")+\n",
    " facet_wrap('Metrica')+\n",
    " theme_bw()\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parte 4: Interpretación de los resultados.\n",
    "Visualicemos el árbol generado, así como el *importance feature* del modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "AD = DecisionTreeClassifier(max_depth=<PROFUNDIDAD_ESCOGIDA>)\n",
    "AD.fit(data, target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Graficamos el árbol.  Si es muy grande, puede cambiar el parámetro 'max_depth' de la función plot_tree (4ta línea) para ver solo los primeros niveles del árbol."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import plot_tree\n",
    "import matplotlib.pyplot as plt\n",
    "plt.figure(figsize=(15,15)) #definiendo el tamaño de la figura\n",
    "plot_tree(AD,filled=True,feature_names=data.columns, max_depth=100)\n",
    "plt.show() #mostrando el árbol"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Otra forma de interpretar el modelo es pidiendo los `.feature_importances_`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "AD.feature_importances_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gráfiquemoslos para ver el resultado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "importance = pd.DataFrame({'importance' : AD.feature_importances_}, index=data.columns)\n",
    "importance_order = importance.sort_values('importance', ascending=True).index.to_list()\n",
    "\n",
    "ggplot(importance, aes(x=importance.index, y='importance'))\\\n",
    "+ scale_x_discrete(limits=importance_order)\\\n",
    "+ geom_col() + coord_flip()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Pregunta**: ¿Coincide con lo visto anteriormente para Naive-Bayes?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python3.9",
   "language": "python",
   "name": "python3.9"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
