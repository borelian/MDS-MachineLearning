{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cargando las librerias requeridas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from plotnine import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "iris = datasets.load_iris() #Cargando el dataset de vino\n",
    "print(iris.keys()) #Analizando las variables que tiene\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "scaled_features = scaler.fit_transform(iris.data) #Transformamos los datos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cohesion y separación\n",
    "Los coeficientes de cohesión y separación deben ser calculados por nosotros mismos. Por lo cual, usando el código de abajo podemos calcularlos en forma rápida."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#Calculando la cohesión y separación\n",
    "from sklearn.cluster import KMeans\n",
    "km = KMeans(n_clusters=2)\n",
    "km=km.fit(scaled_features)\n",
    "\n",
    "tempCohesion=0\n",
    "for j in range(km.n_clusters):\n",
    "    #Calculando el centro del clusters\n",
    "    #centroCluster=np.mean(scaled_features[km.labels_==j,:],axis=0)\n",
    "    centroCluster=km.cluster_centers_[j,:]\n",
    "    #Calculando la cohesion para ese cluster\n",
    "    tempCohesion+=np.square(scaled_features[km.labels_==j,:]-centroCluster).sum()\n",
    "print(\"Cohesión: \",tempCohesion)\n",
    "print(\"Inertia: \",km.inertia_) #For k-means\n",
    "\n",
    "tempSeparacion=0\n",
    "#Calculando el centro de los datos\n",
    "centroData=np.mean(scaled_features,axis=0)\n",
    "for j in range(km.n_clusters):\n",
    "    #Calculando el centro del clusters\n",
    "    #centroCluster=np.mean(scaled_features[km.labels_==j,:],axis=0)\n",
    "    centroCluster=km.cluster_centers_[j,:]\n",
    "    #Calculando la separación para ese cluster\n",
    "    tempSeparacion+=(km.labels_==j).sum()*np.square(centroCluster-centroData).sum()\n",
    "print(\"Separación: \",tempSeparacion)\n",
    "print(\"Separación + cohesión: \",tempSeparacion+tempCohesion)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Coeficiente de silhouette \n",
    "\n",
    "El coeficiente de silhouette promedio se calcula con la función silhouette_score del paquete sklearn.metrics.<br>\n",
    "silhouette_score(X, labels, metric='euclidean', sample_size=None, random_state=None)<br><br>\n",
    "Parámetros\n",
    "* X: set de datos o matriz de distancia.\n",
    "* labels: etiquetas de los clusters.\n",
    "* metric: métrica de distancia a utilizar, si X es una matriz de distancia se utiliza \"precomputed\".\n",
    "* sample_size: número de puntos (para grandes bases de datos).\n",
    "* random_state: inicialización del número aleatorio para el sampleo.<br><br>\n",
    "\n",
    "De igual manera, la función silhouette_samples calcula el coeficiente de Silhouette para cada punto por separado<br>\n",
    "silhouette_samples(X, labels, metric='euclidean')\n",
    "<br><br>\n",
    "Parámetros\n",
    "* X: set de datos o matriz de distancia.\n",
    "* labels: etiquetas de los clusters.\n",
    "* metric: métrica de distancia a utilizar, si X es una matriz de distancia se utiliza \"precomputed\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_samples, silhouette_score\n",
    "numClusters=3\n",
    "km = KMeans(n_clusters=numClusters)\n",
    "km=km.fit(scaled_features)\n",
    "\n",
    "#Calculando el coeficiente de Silhouette para cada muestra\n",
    "sample_silhouette_values = silhouette_samples(scaled_features, km.labels_)\n",
    "print(sample_silhouette_values)\n",
    "\n",
    "#Calculando el coeficiente de silhouette promedio\n",
    "silhouette_avg = silhouette_score(scaled_features, km.labels_)\n",
    "print(\"Para n_clusters =\", numClusters,\"el coeficiente promedio de silhouette_score es:\", silhouette_avg)\n",
    "print(\"Equivalente a \",np.average(silhouette_samples(scaled_features, km.labels_)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Visualizando los coeficientes de Silhouette\n",
    "tempDF = pd.DataFrame(columns=['SC','labels'])\n",
    "for i in range(km.n_clusters):\n",
    "    #Ordenando los valores de cada cluster\n",
    "    tempData=np.sort(sample_silhouette_values[km.labels_==i])[::-1]#para mostrarlos de menor a mayor\n",
    "    tempDF2=pd.DataFrame(tempData,columns=[\"SC\"])\n",
    "    tempDF2[\"labels\"]=i\n",
    "    #Agregandolo a un nuevo data frame\n",
    "    tempDF=tempDF.append(tempDF2)\n",
    "tempDF.reset_index()\n",
    "(ggplot(tempDF)+aes(y=\"SC\",x=range(150),color=\"factor(labels)\")\n",
    " +geom_col(show_legend=False)+coord_flip()+theme_bw()\n",
    " +labs(y=\"Coeficiente de Silhouette\")+theme(axis_text_y=element_blank(),axis_ticks_major_y=element_blank())\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Buscando el mejor número de clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import silhouette_samples, silhouette_score\n",
    "maxK=11\n",
    "sse = np.zeros(maxK-2) # NO USAR []\n",
    "CS = np.zeros(maxK-2) # NO USAR []\n",
    "for k in range(2, maxK):\n",
    "    kmeans = KMeans(n_clusters=k)\n",
    "    kmeans.fit(scaled_features)\n",
    "    sse[k-2]=kmeans.inertia_ #NO USAR sse.append(kmeans.inertia_)\n",
    "    silhouette_avg = silhouette_score(scaled_features, kmeans.labels_)\n",
    "    CS[k-2]=silhouette_avg #NO USAR CS.append(silhouette_avg)\n",
    "    \n",
    "\n",
    "tempData=pd.DataFrame(range(2, maxK),columns=[\"K\"])\n",
    "tempData[\"sse\"]=sse\n",
    "tempData[\"CS\"]=CS\n",
    "print(ggplot(tempData)+aes(x=\"K\",y=\"sse\")+theme_bw()+geom_line()+labs(x=\"Número de clusters\",y=\"WCD\")+scale_x_continuous(breaks=range(1,maxK)))\n",
    "print(ggplot(tempData)+aes(x=\"K\",y=\"CS\")+theme_bw()+geom_line()+labs(x=\"Número de clusters\",y=\"Coeficiente de Silhouette promedio\")+scale_x_continuous(breaks=range(1,maxK)))    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hopkins statistics\n",
    "La función hopkins del paquete pyclustertend permite calcular la tendencia de hopkins en forma rápida. Sin embargo, tiene problemas con largos set de datos.<br>\n",
    "\n",
    "La función recibe solo dos parámetros, los datos y el número de puntos aleatorios a generar.<br>\n",
    "hopkins(dataset,numSample)\n",
    "\n",
    "Recuerde calcula 1 menos el valor del test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importando la función de hopkins\n",
    "from pyclustertend import hopkins\n",
    "print(\"Coeficiente de Hopkins, datos de iris escalados:\",1-hopkins(scaled_features,50))\n",
    "\n",
    "#Generando 150 datos aleatorios\n",
    "tempData=np.random.uniform(low=0,high=1,size=(150,2))\n",
    "tempData=pd.DataFrame(tempData,columns=[\"X1\",\"X2\"])\n",
    "print(ggplot(tempData)+aes(x=\"X1\",y=\"X2\")+geom_point())\n",
    "print(\"Coeficiente de Hopkins, datos aleatorios:\",1-hopkins(tempData,30))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluación matriz de distancia\n",
    "Para evaluar la matriz de distancia entre los puntos, utilizaremos las funciones vat del paquete pyclustertend. Tal como en el caso anterior, se tiene problemas con largos set de datos.<br>\n",
    "\n",
    "La función recibe un set de datos y despliega la matriz de distancia en forma ordenada para mejorar su visualización.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Instalando el modulo requerido\n",
    "#import sys\n",
    "#!{sys.executable} -m pip install pyclustertend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyclustertend import vat\n",
    "vat(scaled_features)\n",
    "vat(tempData)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
